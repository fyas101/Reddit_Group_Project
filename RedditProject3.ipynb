{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fyas101/Reddit_Group_Project/blob/main/RedditProject3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z00-pc6mQ1Yt",
        "outputId": "1102dde9-205e-4bf9-f7c0-e3a075b23492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ContentCategorizer (tone + misinformation)': {'tone': 'informative', 'misinformation': {'is_misinformation': False, 'matched_keywords': []}}, 'PostCleaner (cleaning + duplicate detection)': {'cleaned': 'good reddit post', 'duplicate': True}, 'EngagementAnalyzer (engagement + weekly stats)': {'total_posts': 1, 'total_upvotes': 10, 'total_comments': 4, 'total_interactions': 14}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3817125610.py:237: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  {\"created_utc\": datetime.utcnow().isoformat(), \"upvotes\": 10, \"comments\": 4, \"category\": \"news\"}\n",
            "/tmp/ipython-input-3817125610.py:93: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  week_start = datetime.utcnow() - timedelta(days=7)\n"
          ]
        }
      ],
      "source": [
        "# Project 3\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Any, List, Dict, Optional\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "\n",
        "\n",
        "# abstract base class\n",
        "\n",
        "class BaseAnalyzer(ABC):\n",
        "\n",
        "    def __init__(self):\n",
        "        self._history: List[Dict[str, Any]] = []\n",
        "\n",
        "    @abstractmethod\n",
        "    def analyze(self, data: Any) -> Any:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def describe(self) -> str:\n",
        "        pass\n",
        "\n",
        "    def add_history(self, record: Dict[str, Any]):\n",
        "        self._history.append(record)\n",
        "\n",
        "    @property\n",
        "    def history(self) -> List[Dict[str, Any]]:\n",
        "        return list(self._history)\n",
        "\n",
        "    @property\n",
        "    def analysis_count(self) -> int:\n",
        "        return len(self._history)\n",
        "\n",
        "\n",
        "# exact project 2 functions\n",
        "\n",
        "def categorize_by_tone(post_text):\n",
        "    if not isinstance(post_text, str):\n",
        "        raise TypeError(\"post_text must be a string\")\n",
        "    clean = post_text.strip()\n",
        "    lower = clean.lower()\n",
        "    if \"yeah right\" in lower or \"sure\" in lower: return \"sarcastic\"\n",
        "    if any(w in lower for w in [\"angry\", \"hate\"]) or clean.isupper(): return \"angry\"\n",
        "    if \"lol\" in lower or \"haha\" in lower: return \"humorous\"\n",
        "    if \"?\" in clean: return \"neutral\"\n",
        "    return \"informative\"\n",
        "\n",
        "def detect_misinformation(post_text, keyword_list=None):\n",
        "    default = [\"rumor\", \"unconfirmed\", \"fake news\", \"scam\", \"hoax\"]\n",
        "    words = keyword_list if keyword_list else default\n",
        "    cleaned = re.sub(r\"http\\S+\", \"\", post_text.lower())\n",
        "    matches = [k for k in words if k in cleaned]\n",
        "    return {\"is_misinformation\": bool(matches), \"matched_keywords\": matches}\n",
        "\n",
        "\n",
        "def clean_post_text(text: str) -> str:\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).lower().strip()\n",
        "\n",
        "\n",
        "def check_duplicate(new_post: str, existing_posts: List[str], threshold: float = 0.9) -> bool:\n",
        "    clean = lambda t: set(clean_post_text(t).split())\n",
        "    nw = clean(new_post)\n",
        "    for p in existing_posts:\n",
        "        pw = clean(p)\n",
        "        if len(nw | pw) == 0:\n",
        "            continue\n",
        "        overlap = len(nw & pw) / len(nw | pw)\n",
        "        if overlap >= threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def track_users(posts):\n",
        "    users = {}\n",
        "    for p in posts:\n",
        "        u = p[\"username\"]\n",
        "        users.setdefault(u, {\n",
        "            \"total_posts\": 0,\n",
        "            \"total_upvotes\": 0,\n",
        "            \"total_comments\": 0,\n",
        "            \"disinformation_posts\": 0\n",
        "        })\n",
        "        users[u][\"total_posts\"] += 1\n",
        "        users[u][\"total_upvotes\"] += p[\"upvotes\"]\n",
        "        users[u][\"total_comments\"] += p[\"comments\"]\n",
        "        if p.get(\"is_disinformation\"):\n",
        "            users[u][\"disinformation_posts\"] += 1\n",
        "    return users\n",
        "\n",
        "def total_interactions_this_week(posts):\n",
        "    week_start = datetime.utcnow() - timedelta(days=7)\n",
        "    up, com = 0, 0\n",
        "    count = 0\n",
        "    for p in posts:\n",
        "        if datetime.fromisoformat(p[\"created_utc\"]) >= week_start:\n",
        "            count += 1\n",
        "            up += p[\"upvotes\"]\n",
        "            com += p[\"comments\"]\n",
        "    return {\n",
        "        \"total_posts\": count,\n",
        "        \"total_upvotes\": up,\n",
        "        \"total_comments\": com,\n",
        "        \"total_interactions\": up + com\n",
        "    }\n",
        "\n",
        "def compare_engagement(posts, group_by=\"category\"):\n",
        "    totals = {}\n",
        "    counts = {}\n",
        "    for p in posts:\n",
        "        grp = p[group_by]\n",
        "        totals[grp] = totals.get(grp, 0) + (p[\"upvotes\"] + p[\"comments\"])\n",
        "        counts[grp] = counts.get(grp, 0) + 1\n",
        "    return {g: round(totals[g]/counts[g], 2) for g in totals}\n",
        "\n",
        "def track_top_posts(posts, n=5):\n",
        "    for p in posts:\n",
        "        p[\"total_interactions\"] = p[\"upvotes\"] + p[\"comments\"]\n",
        "    return sorted(posts, key=lambda x: x[\"total_interactions\"], reverse=True)[:n]\n",
        "\n",
        "\n",
        "# Project 2 Classes with ineretance\n",
        "class ContentCategorizer(BaseAnalyzer):\n",
        "\n",
        "    def __init__(self, custom_keywords=None):\n",
        "        super().__init__()\n",
        "        self._keywords = custom_keywords\n",
        "\n",
        "    def analyze(self, post_text: str) -> Dict[str, Any]:\n",
        "        tone = categorize_by_tone(post_text)\n",
        "        misinfo = detect_misinformation(post_text)\n",
        "        result = {\"tone\": tone, \"misinformation\": misinfo}\n",
        "        self.add_history({\"text\": post_text[:40], \"result\": result})\n",
        "        return result\n",
        "\n",
        "    def describe(self) -> str:\n",
        "        return \"ContentCategorizer (tone + misinformation)\"\n",
        "\n",
        "\n",
        "class PostCleaner(BaseAnalyzer):\n",
        "\n",
        "    def __init__(self, post_text: str, existing=None):\n",
        "        super().__init__()\n",
        "        self.post_text = post_text\n",
        "        self.existing = existing or []\n",
        "\n",
        "    def analyze(self, _=None) -> Dict[str, Any]:\n",
        "        cleaned = clean_post_text(self.post_text)\n",
        "        dup = check_duplicate(self.post_text, self.existing)\n",
        "        result = {\"cleaned\": cleaned, \"duplicate\": dup}\n",
        "        self.add_history(result)\n",
        "        return result\n",
        "\n",
        "    def describe(self) -> str:\n",
        "        return \"PostCleaner (cleaning + duplicate detection)\"\n",
        "\n",
        "\n",
        "class UserTracker(BaseAnalyzer):\n",
        "\n",
        "    def __init__(self, posts=None):\n",
        "        super().__init__()\n",
        "        self.posts = posts or []\n",
        "\n",
        "    def analyze(self, username: str) -> Dict[str, Any]:\n",
        "        stats = track_users(self.posts)\n",
        "        result = stats.get(username, {})\n",
        "        self.add_history({\"username\": username, \"stats\": result})\n",
        "        return result\n",
        "\n",
        "    def describe(self) -> str:\n",
        "        return \"UserTracker (user statistics + reliability)\"\n",
        "\n",
        "\n",
        "class EngagementAnalyzer(BaseAnalyzer):\n",
        "\n",
        "    def __init__(self, posts=None):\n",
        "        super().__init__()\n",
        "        self.posts = posts or []\n",
        "\n",
        "    def analyze(self, _=None) -> Dict[str, Any]:\n",
        "        weekly = total_interactions_this_week(self.posts)\n",
        "        self.add_history(weekly)\n",
        "        return weekly\n",
        "\n",
        "    def describe(self) -> str:\n",
        "        return \"EngagementAnalyzer (engagement + weekly stats)\"\n",
        "\n",
        "\n",
        "class MetadataAnalyzer(BaseAnalyzer):\n",
        "\n",
        "    def __init__(self, metadata=None):\n",
        "        super().__init__()\n",
        "        self.metadata = metadata or []\n",
        "\n",
        "    def analyze(self, _=None) -> Dict[str, Any]:\n",
        "        summary = {}\n",
        "        for record in self.metadata:\n",
        "            for key, val in record.items():\n",
        "                summary.setdefault(key, {})\n",
        "                v = str(val)\n",
        "                summary[key][v] = summary[key].get(v, 0) + 1\n",
        "\n",
        "        self.add_history({\"records\": len(self.metadata)})\n",
        "        return summary\n",
        "\n",
        "    def describe(self) -> str:\n",
        "        return \"MetadataAnalyzer (metadata frequencies)\"\n",
        "\n",
        "\n",
        "#polymorhpic comoposition\n",
        "\n",
        "class RedditAnalysisManager:\n",
        "    def __init__(self):\n",
        "        self.analyzers: List[BaseAnalyzer] = []\n",
        "\n",
        "    def add_analyzer(self, analyzer: BaseAnalyzer):\n",
        "        self.analyzers.append(analyzer)\n",
        "\n",
        "    def run_all(self, data: Any) -> Dict[str, Any]:\n",
        "        \"\"\"Polymorphic execution.\"\"\"\n",
        "        results = {}\n",
        "        for analyzer in self.analyzers:\n",
        "            results[analyzer.describe()] = analyzer.analyze(data)\n",
        "        return results\n",
        "\n",
        "\n",
        "#demo for reddit data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    mgr = RedditAnalysisManager()\n",
        "\n",
        "    mgr.add_analyzer(ContentCategorizer())\n",
        "    mgr.add_analyzer(PostCleaner(\"Good Reddit Post!\", [\"good reddit post!\"]))\n",
        "    mgr.add_analyzer(EngagementAnalyzer([\n",
        "        {\"created_utc\": datetime.utcnow().isoformat(), \"upvotes\": 10, \"comments\": 4, \"category\": \"news\"}\n",
        "    ]))\n",
        "\n",
        "    print(mgr.run_all(\"Good Reddit Post!\"))\n"
      ]
    }
  ]
}