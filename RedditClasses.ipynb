{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fyas101/Reddit_Group_Project/blob/main/RedditClasses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArUX6aiHXIaX"
      },
      "outputs": [],
      "source": [
        "# This class utilized the group #3 functions from module 1 which are categorizing by tone, detecting misinformation, and extracting category keywords.\n",
        "\n",
        "import re\n",
        "from typing import Optional, Dict, List\n",
        "\n",
        "# Categorization by tone function from Project 1\n",
        "def categorize_by_tone(post_text):\n",
        "    if not isinstance(post_text, str):\n",
        "        raise TypeError(\"post_text must be a string\")\n",
        "\n",
        "    clean_text = post_text.strip()\n",
        "    if not clean_text:\n",
        "        raise ValueError(\"post_text cannot be empty\")\n",
        "\n",
        "    lower_text = clean_text.lower()\n",
        "\n",
        "    # Basic heuristic keywords and cues\n",
        "    anger_cues = ['!', 'angry', 'hate', 'worst', 'terrible']\n",
        "    sarcasm_cues = ['yeah right', 'sure', 'totally']\n",
        "    humor_cues = ['lol', 'funny', 'haha']\n",
        "    uncertainty_cues = ['maybe', 'not sure', 'idk', 'perhaps']\n",
        "\n",
        "    # Rule-based detection\n",
        "    if any(cue in lower_text for cue in sarcasm_cues):\n",
        "        return 'sarcastic'\n",
        "    elif any(cue in lower_text for cue in anger_cues) and clean_text.isupper():\n",
        "        return 'angry'\n",
        "    elif any(cue in lower_text for cue in humor_cues):\n",
        "        return 'humorous'\n",
        "    elif any(cue in lower_text for cue in uncertainty_cues):\n",
        "        return 'uncertain'\n",
        "    elif '?' in clean_text:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'informative'\n",
        "\n",
        "# Detection of misinformation function from Project 1\n",
        "def detect_misinformation(post_text, keyword_list=None):\n",
        "    if not isinstance(post_text, str):\n",
        "        raise TypeError(\"post_text must be a string.\")\n",
        "    if keyword_list is not None and not (isinstance(keyword_list, list) and all(isinstance(k, str) for k in keyword_list)):\n",
        "        raise TypeError(\"keyword_list must be a list of strings or None.\")\n",
        "\n",
        "    # Default keywords often associated with misinformation\n",
        "    default_keywords = [\n",
        "        \"rumor\", \"unconfirmed\", \"heard\", \"confirmed??\", \"sources say\", \"reportedly\", \"breaking\", \"shocking\",\n",
        "        \"can't believe\", \"conspiracy\", \"fake news\", \"scam\", \"hoax\", \"allegedly\"\n",
        "    ]\n",
        "\n",
        "    keywords = keyword_list if keyword_list else default_keywords\n",
        "\n",
        "    # Clean post text for matching\n",
        "    cleaned_text = post_text.lower()\n",
        "    cleaned_text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", cleaned_text)  # remove URLs\n",
        "    cleaned_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", cleaned_text)  # remove punctuation\n",
        "\n",
        "    matched = [kw for kw in keywords if kw in cleaned_text]\n",
        "\n",
        "    return {\n",
        "        \"is_misinformation\": bool(matched),\n",
        "        \"matched_keywords\": matched\n",
        "    }\n",
        "\n",
        "# Extraction of category keywords function from project 1\n",
        "def extract_category_keywords(categories):\n",
        "    # Returns a dictionary that maps each category to example keywords after given a list of categories.\n",
        "\n",
        "    categegory_keywords = {\n",
        "        \"humor\": [\"lol\", \"lmao\", \"haha\", \"joke\"],\n",
        "        \"random\": [\"idk\", \"random\", ],\n",
        "        \"news\": [\"alert\", \"news\", \"update\", \"announcement\", \"diamondback\", \"report\"],\n",
        "        \"academics\": [\"class\", \"classes\", \"exam\", \"professor\", \"grade\", \"gpa\", \"study\", \"midterm\", \"final\", \"project\"],\n",
        "        \"advice\": [\"reccomend\", \"tips\", \"help\", \"should I\", \"question\"],\n",
        "        \"social\": [\"party\", \"hangout\", \"movie\", \"homecoming\", \"game\"],\n",
        "    }\n",
        "    # Lists of categories with assigned example keywords\n",
        "    return {cat: categegory_keywords.get(cat, []) for cat in categories}\n",
        "\n",
        "\n",
        "def normalize_word(word):\n",
        "    # helps to make keywords lowercase as well as clean up extra space\n",
        "    return word.strip().lower()\n",
        "\n",
        "# Created class for Group #3 Organization and Categorizing\n",
        "class ContentCategorizer:\n",
        "    def __init__(self, custom_keywords: Optional[Dict[str, List[str]]] = None):\n",
        "        if custom_keywords is not None:\n",
        "            if not isinstance(custom_keywords, dict):\n",
        "                raise TypeError(\"custom_keywords must be a dictionary\")\n",
        "            for key, value in custom_keywords.items():\n",
        "                if not isinstance(key, str):\n",
        "                    raise TypeError(\"All category names must be strings\")\n",
        "                if not isinstance(value, list):\n",
        "                    raise TypeError(f\"Keywords for category '{key}' must be a list\")\n",
        "\n",
        "        self._category_keywords = {\n",
        "            \"humor\": [\"lol\", \"lmao\", \"haha\", \"joke\", \"funny\"],\n",
        "            \"random\": [\"idk\", \"random\", \"whatever\"],\n",
        "            \"news\": [\"alert\", \"news\", \"update\", \"announcement\", \"diamondback\", \"report\"],\n",
        "            \"academics\": [\"class\", \"classes\", \"exam\", \"professor\", \"grade\",\n",
        "                         \"gpa\", \"study\", \"midterm\", \"final\", \"project\", \"homework\"],\n",
        "            \"advice\": [\"recommend\", \"tips\", \"help\", \"should i\", \"question\", \"advice\"],\n",
        "            \"social\": [\"party\", \"hangout\", \"movie\", \"homecoming\", \"game\", \"event\"]\n",
        "        }\n",
        "\n",
        "        if custom_keywords:\n",
        "            for category, keywords in custom_keywords.items():\n",
        "                self._category_keywords[category] = [kw.lower().strip() for kw in keywords]\n",
        "\n",
        "        self._misinformation_keywords = [\n",
        "            \"rumor\", \"unconfirmed\", \"heard\", \"confirmed??\", \"sources say\",\n",
        "            \"reportedly\", \"breaking\", \"shocking\", \"can't believe\",\n",
        "            \"conspiracy\", \"fake news\", \"scam\", \"hoax\", \"allegedly\"\n",
        "        ]\n",
        "        self._analysis_history = []\n",
        "\n",
        "    @property\n",
        "    def category_keywords(self) -> Dict[str, List[str]]:\n",
        "        return dict(self._category_keywords)\n",
        "\n",
        "    def analysis_count(self) -> int:\n",
        "        return len(self._analysis_history)\n",
        "\n",
        "    # First method\n",
        "    def analyze_tone(self, post_text: str) -> str:\n",
        "        # Call Project 1 Function\n",
        "        tone = categorize_by_tone(post_text)\n",
        "        self._analysis_history.append({\n",
        "            'type': 'tone',\n",
        "            'text': post_text[:50],\n",
        "            'result': tone\n",
        "        })\n",
        "        return tone\n",
        "\n",
        "    # Second method\n",
        "    def check_for_misinformation(self, post_text: str,\n",
        "                                 custom_keywords: Optional[List[str]] = None) -> Dict[str, any]:\n",
        "        keywords = custom_keywords if custom_keywords else self._misinformation_keywords\n",
        "        # Call project 1 function\n",
        "        result = detect_misinformation(post_text, keywords)\n",
        "        self._analysis_history.append({\n",
        "            'type': 'misinformation',\n",
        "            'text': post_text[:50],\n",
        "            'result': result['is_misinformation']\n",
        "        })\n",
        "\n",
        "        return result\n",
        "\n",
        "# Third method\n",
        "    def get_category_keywords(self, categories: Optional[List[str]] = None) -> Dict[str, List[str]]:\n",
        "        if categories is None:\n",
        "            categories = list(self._category_keywords.keys())\n",
        "        for cat in categories:\n",
        "            if cat not in self._category_keywords:\n",
        "                raise ValueError(f\"Category '{cat}' does not exist\")\n",
        "        return {cat: self._category_keywords[cat] for cat in categories}\n",
        "\n",
        "# Fourth method\n",
        "    def categorize_content(self, post_text: str) -> str:\n",
        "        if not isinstance(post_text, str):\n",
        "            raise TypeError(\"post_text must be a string\")\n",
        "\n",
        "        cleaned_text = post_text.lower()\n",
        "\n",
        "# Fifth method\n",
        "    def analyze_post(self, post_text: str, check_misinfo: bool = True) -> Dict[str, any]:\n",
        "        analysis = {\n",
        "            'tone': self.analyze_tone(post_text),\n",
        "            'category': self.categorize_content(post_text)\n",
        "        }\n",
        "\n",
        "        if check_misinfo:\n",
        "            analysis['misinformation'] = self.check_for_misinformation(post_text)\n",
        "\n",
        "        return analysis\n",
        "\n",
        "  # Group 3 functions class creation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any\n",
        "\n",
        "#track users function from project 1\n",
        "def track_users(posts_data):\n",
        "    users_summary = {}\n",
        "    for post in posts_data:\n",
        "        user = post[\"username\"]\n",
        "        if user not in users_summary:\n",
        "            users_summary[user] = {\n",
        "                \"total_posts\": 0,\n",
        "                \"total_upvotes\": 0,\n",
        "                \"total_comments\": 0,\n",
        "                \"disinformation_posts\": 0\n",
        "            }\n",
        "        users_summary[user][\"total_posts\"] += 1\n",
        "        users_summary[user][\"total_upvotes\"] += post[\"upvotes\"]\n",
        "        users_summary[user][\"total_comments\"] += post[\"comments\"]\n",
        "        if post.get(\"is_disinformation\"):\n",
        "            users_summary[user][\"disinformation_posts\"] += 1\n",
        "    return users_summary\n",
        "\n",
        "#top posters function from project 1\n",
        "def top_posters_list(posts_data, top_n=10):\n",
        "    user_stats = {}\n",
        "    for post in posts_data:\n",
        "        user = post[\"username\"]\n",
        "        if user not in user_stats:\n",
        "            user_stats[user] = {\"total_posts\": 0, \"total_engagement\": 0}\n",
        "        user_stats[user][\"total_posts\"] += 1\n",
        "        user_stats[user][\"total_engagement\"] += post[\"upvotes\"] + post[\"comments\"]\n",
        "\n",
        "    ranked = [{\"username\": u, **s} for u, s in user_stats.items()]\n",
        "    ranked.sort(key=lambda x: x[\"total_engagement\"], reverse=True)\n",
        "    return ranked[:top_n]\n",
        "\n",
        "#interaction rate function from project 1\n",
        "def interaction_rate(post_data):\n",
        "    views = post_data[\"views\"]\n",
        "    if views <= 0:\n",
        "        raise ValueError(\"views must be greater than zero\")\n",
        "    total = post_data[\"upvotes\"] + post_data[\"comments\"]\n",
        "    return round((total / views) * 100, 2)\n",
        "\n",
        "class UserTracker:\n",
        "    \"\"\"tracks user activity and statistics.\"\"\"\n",
        "\n",
        "    def __init__(self, posts_data: List[Dict] = None):\n",
        "        if posts_data and not isinstance(posts_data, list):\n",
        "            raise TypeError(\"posts_data must be a list\")\n",
        "        self._posts_data = posts_data or []\n",
        "\n",
        "    @property\n",
        "    def posts_data(self):\n",
        "        return self._posts_data\n",
        "\n",
        "    @property\n",
        "    def user_count(self):\n",
        "        return len(set(p[\"username\"] for p in self._posts_data if \"username\" in p))\n",
        "\n",
        "    #method 1: get all user statistics\n",
        "    def get_user_stats(self):\n",
        "        if not self._posts_data:\n",
        "            return {}\n",
        "        return track_users(self._posts_data)\n",
        "\n",
        "    #method 2: get top posters\n",
        "    def get_top_posters(self, top_n=10):\n",
        "        if not self._posts_data:\n",
        "            return []\n",
        "        return top_posters_list(self._posts_data, top_n)\n",
        "\n",
        "    #method 3: calculate user interaction rate\n",
        "    def user_interaction_rate(self, username):\n",
        "        user_posts = [p for p in self._posts_data if p.get(\"username\") == username and \"views\" in p]\n",
        "        if not user_posts:\n",
        "            raise ValueError(f\"no posts with views for {username}\")\n",
        "        rates = [interaction_rate(p) for p in user_posts]\n",
        "        return round(sum(rates) / len(rates), 2)\n",
        "\n",
        "    #method 4: get user reliability score\n",
        "    def user_reliability(self, username):\n",
        "        stats = self.get_user_stats()\n",
        "        if username not in stats:\n",
        "            raise ValueError(f\"user {username} not found\")\n",
        "        total = stats[username][\"total_posts\"]\n",
        "        misinfo = stats[username][\"disinformation_posts\"]\n",
        "        return round(((total - misinfo) / total) * 100, 2) if total > 0 else 100.0\n",
        "\n",
        "    #method 5: add new post\n",
        "    def add_post(self, post_data):\n",
        "        if not isinstance(post_data, dict):\n",
        "            raise TypeError(\"post_data must be a dictionary\")\n",
        "        self._posts_data.append(post_data)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"usertracker: {self.user_count} users, {len(self._posts_data)} posts\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"usertracker(posts_data=[{len(self._posts_data)} posts])\""
      ],
      "metadata": {
        "id": "81utLX8kZodn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict\n",
        "\n",
        "#weekly interactions function from project 1\n",
        "def total_interactions_this_week(posts_data):\n",
        "    week_start = datetime.utcnow() - timedelta(days=7)\n",
        "    total_upvotes = 0\n",
        "    total_comments = 0\n",
        "    post_count = 0\n",
        "\n",
        "    for post in posts_data:\n",
        "        post_date = datetime.fromisoformat(post[\"created_utc\"])\n",
        "        if post_date >= week_start:\n",
        "            post_count += 1\n",
        "            total_upvotes += post[\"upvotes\"]\n",
        "            total_comments += post[\"comments\"]\n",
        "\n",
        "    return {\n",
        "        \"total_posts\": post_count,\n",
        "        \"total_upvotes\": total_upvotes,\n",
        "        \"total_comments\": total_comments,\n",
        "        \"total_interactions\": total_upvotes + total_comments\n",
        "    }\n",
        "\n",
        "#compare engagement function from project 1\n",
        "def compare_engagement(posts_data, group_by=\"category\"):\n",
        "    engagement_by_group = {}\n",
        "    count_by_group = {}\n",
        "\n",
        "    for post in posts_data:\n",
        "        group = post[group_by]\n",
        "        engagement = post.get(\"upvotes\", 0) + post.get(\"comments\", 0)\n",
        "        engagement_by_group[group] = engagement_by_group.get(group, 0) + engagement\n",
        "        count_by_group[group] = count_by_group.get(group, 0) + 1\n",
        "\n",
        "    return {g: round(engagement_by_group[g] / count_by_group[g], 2) for g in engagement_by_group}\n",
        "\n",
        "#top posts function from project 1\n",
        "def track_top_posts(posts_data, top_n=5):\n",
        "    for post in posts_data:\n",
        "        post[\"total_interactions\"] = post[\"upvotes\"] + post[\"comments\"]\n",
        "    sorted_posts = sorted(posts_data, key=lambda x: x[\"total_interactions\"], reverse=True)\n",
        "    return sorted_posts[:top_n]\n",
        "\n",
        "class EngagementAnalyzer:\n",
        "    \"\"\"analyzes post engagement and trends.\"\"\"\n",
        "\n",
        "    def __init__(self, posts_data: List[Dict] = None):\n",
        "        if posts_data and not isinstance(posts_data, list):\n",
        "            raise TypeError(\"posts_data must be a list\")\n",
        "        self._posts_data = posts_data or []\n",
        "\n",
        "    @property\n",
        "    def posts_data(self):\n",
        "        return self._posts_data\n",
        "\n",
        "    @property\n",
        "    def total_engagement(self):\n",
        "        return sum(p.get(\"upvotes\", 0) + p.get(\"comments\", 0) for p in self._posts_data)\n",
        "\n",
        "    #method 1: get weekly summary\n",
        "    def weekly_summary(self):\n",
        "        if not self._posts_data:\n",
        "            return {\"total_posts\": 0, \"total_upvotes\": 0, \"total_comments\": 0, \"total_interactions\": 0}\n",
        "        return total_interactions_this_week(self._posts_data)\n",
        "\n",
        "    #method 2: compare by category\n",
        "    def compare_categories(self, group_by=\"category\"):\n",
        "        if not self._posts_data:\n",
        "            return {}\n",
        "        return compare_engagement(self._posts_data, group_by)\n",
        "\n",
        "    #method 3: get top posts\n",
        "    def top_posts(self, top_n=5):\n",
        "        if not self._posts_data:\n",
        "            return []\n",
        "        return track_top_posts(self._posts_data, top_n)\n",
        "\n",
        "    #method 4: calculate average engagement\n",
        "    def avg_engagement(self):\n",
        "        if not self._posts_data:\n",
        "            return 0.0\n",
        "        return round(self.total_engagement / len(self._posts_data), 2)\n",
        "\n",
        "    #method 5: add new post\n",
        "    def add_post(self, post_data):\n",
        "        if not isinstance(post_data, dict):\n",
        "            raise TypeError(\"post_data must be a dictionary\")\n",
        "        self._posts_data.append(post_data)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"engagementanalyzer: {len(self._posts_data)} posts, {self.total_engagement} engagement\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"engagementanalyzer(posts_data=[{len(self._posts_data)} posts])\""
      ],
      "metadata": {
        "id": "w141aLghqO6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "#Group 1 Cleaning function original function\n",
        "def clean_post_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        raise TypeError(\"Input must be a string.\")\n",
        "\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[@#]\\w+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "#Group 1 removing duplicates orignal function\n",
        "def check_duplicate(new_post: str, existing_posts: List[str], similarity_threshold: float = 0.9) -> bool:\n",
        "    if not isinstance(new_post, str):\n",
        "        raise TypeError(\"new_post must be a string.\")\n",
        "    if not isinstance(existing_posts, list) or not all(isinstance(p, str) for p in existing_posts):\n",
        "        raise TypeError(\"existing_posts must be a list of strings.\")\n",
        "    if not (0 <= similarity_threshold <= 1):\n",
        "        raise ValueError(\"similarity_threshold must be between 0 and 1.\")\n",
        "\n",
        "    def _clean_text(text):\n",
        "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text).lower()\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        return text\n",
        "\n",
        "    new_post_clean = _clean_text(new_post)\n",
        "    new_words = set(new_post_clean.split())\n",
        "\n",
        "    for post in existing_posts:\n",
        "        existing_clean = _clean_text(post)\n",
        "        existing_words = set(existing_clean.split())\n",
        "        if not existing_words:\n",
        "            continue\n",
        "\n",
        "        overlap = len(new_words & existing_words) / len(new_words | existing_words)\n",
        "        if overlap >= similarity_threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "class PostCleaner:\n",
        "\n",
        "    def __init__(self, post_text: str, existing_posts: List[str] = None, similarity_threshold: float = 0.9):\n",
        "        if not isinstance(post_text, str):\n",
        "            raise TypeError(\"post_text must be a string.\")\n",
        "        if existing_posts is not None:\n",
        "            if not isinstance(existing_posts, list) or not all(isinstance(p, str) for p in existing_posts):\n",
        "                raise TypeError(\"existing_posts must be a list of strings.\")\n",
        "        if not (0 <= similarity_threshold <= 1):\n",
        "            raise ValueError(\"similarity_threshold must be between 0 and 1.\")\n",
        "\n",
        "        self._post_text = post_text\n",
        "        self._existing_posts = existing_posts or []\n",
        "        self._similarity_threshold = similarity_threshold\n",
        "        self._clean_history: List[Dict[str, Any]] = []\n",
        "\n",
        "    @property\n",
        "    def post_text(self) -> str:\n",
        "        return self._post_text\n",
        "\n",
        "    @property\n",
        "    def existing_posts(self) -> List[str]:\n",
        "        return list(self._existing_posts)\n",
        "\n",
        "    @property\n",
        "    def similarity_threshold(self) -> float:\n",
        "        return self._similarity_threshold\n",
        "\n",
        "    # Method 1\n",
        "    def clean_text(self) -> str:\n",
        "        cleaned = clean_post_text(self._post_text)\n",
        "        self._clean_history.append({\"type\": \"clean\", \"original\": self._post_text[:50], \"result\": cleaned})\n",
        "        return cleaned\n",
        "\n",
        "    # Method 2\n",
        "    def is_duplicate(self) -> bool:\n",
        "        result = check_duplicate(self._post_text, self._existing_posts, self._similarity_threshold)\n",
        "        self._clean_history.append({\"type\": \"duplicate\", \"text\": self._post_text[:50], \"result\": result})\n",
        "        return result\n",
        "\n",
        "    # Method 3\n",
        "    def summarize_post(self) -> Dict[str, Any]:\n",
        "        cleaned = self.clean_text()\n",
        "        duplicate = self.is_duplicate()\n",
        "        return {\"cleaned_text\": cleaned, \"is_duplicate\": duplicate}\n",
        "\n",
        "    # Method 4\n",
        "    def history_count(self) -> int:\n",
        "        return len(self._clean_history)\n"
      ],
      "metadata": {
        "id": "UscliQ1fwgon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any\n",
        "\n",
        "#Group 2 original function\n",
        "def total_metadata_type(posts_metadata: List[Dict[str, Any]]) -> Dict[str, Dict[str, int]]:\n",
        "    if not isinstance(posts_metadata, list):\n",
        "        raise TypeError(\"posts_metadata must be a list of dictionaries.\")\n",
        "    if not posts_metadata:\n",
        "        raise ValueError(\"posts_metadata cannot be empty.\")\n",
        "    if not all(isinstance(post, dict) for post in posts_metadata):\n",
        "        raise TypeError(\"Each element must be a dictionary.\")\n",
        "\n",
        "    summary = {}\n",
        "    for post in posts_metadata:\n",
        "        for key, value in post.items():\n",
        "            summary.setdefault(key, {})\n",
        "            str_value = str(value)\n",
        "            summary[key][str_value] = summary[key].get(str_value, 0) + 1\n",
        "    return summary\n",
        "\n",
        "#Group 2 original function\n",
        "def post_type(post_data: Dict[str, Any]) -> str:\n",
        "    if not isinstance(post_data, dict):\n",
        "        raise TypeError(\"post_data must be a dictionary.\")\n",
        "\n",
        "    if post_data.get(\"post_hint\") == \"image\":\n",
        "        return \"Pictures\"\n",
        "\n",
        "    url = post_data.get(\"url\", \"\").lower()\n",
        "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\", \".webp\"]\n",
        "    if any(url.endswith(ext) for ext in image_extensions):\n",
        "        return \"Pictures\"\n",
        "\n",
        "    selftext = post_data.get(\"selftext\", \"\")\n",
        "    if isinstance(selftext, str) and selftext.strip():\n",
        "        return \"Words\"\n",
        "\n",
        "    return \"Other\"\n",
        "\n",
        "#Group 2 original function\n",
        "def analyze_post_lengths_by_category(posts: List[str], category_keywords: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "    if not isinstance(posts, list):\n",
        "        raise TypeError(\"posts must be a list of strings.\")\n",
        "    if not isinstance(category_keywords, dict):\n",
        "        raise TypeError(\"category_keywords must be a dictionary.\")\n",
        "\n",
        "    categorized = {}\n",
        "    for post in posts:\n",
        "        if not isinstance(post, str) or not post.strip():\n",
        "            continue\n",
        "        text = post.lower()\n",
        "        category = \"other\"\n",
        "        for cat, keywords in category_keywords.items():\n",
        "            if any(kw in text for kw in keywords):\n",
        "                category = cat\n",
        "                break\n",
        "        word_count = len(post.split())\n",
        "        categorized.setdefault(category, []).append((post, word_count))\n",
        "\n",
        "    results = {}\n",
        "    for cat, items in categorized.items():\n",
        "        lengths = [length for _, length in items]\n",
        "        avg_length = round(sum(lengths) / len(lengths), 2)\n",
        "        results[cat] = {\"posts\": items, \"average_length\": avg_length}\n",
        "    return results\n",
        "\n",
        "\n",
        "class MetadataAnalyzer:\n",
        "    def __init__(self, posts_metadata: List[Dict[str, Any]] = None):\n",
        "        if posts_metadata is not None:\n",
        "            if not isinstance(posts_metadata, list) or not all(isinstance(p, dict) for p in posts_metadata):\n",
        "                raise TypeError(\"posts_metadata must be a list of dictionaries.\")\n",
        "        self._posts_metadata = posts_metadata or []\n",
        "        self._analysis_history: List[Dict[str, Any]] = []\n",
        "\n",
        "    @property\n",
        "    def posts_metadata(self) -> List[Dict[str, Any]]:\n",
        "        return list(self._posts_metadata)\n",
        "\n",
        "    @property\n",
        "    def analysis_count(self) -> int:\n",
        "        return len(self._analysis_history)\n",
        "\n",
        "    # Method 1\n",
        "    def summarize_metadata(self, posts_metadata: List[Dict[str, Any]] = None) -> Dict[str, Dict[str, int]]:\n",
        "        data = posts_metadata or self._posts_metadata\n",
        "        summary = total_metadata_type(data)\n",
        "        self._analysis_history.append({\"type\": \"summary\", \"records\": len(data)})\n",
        "        return summary\n",
        "\n",
        "    # Method 2\n",
        "    def classify_post(self, post_data: Dict[str, Any]) -> str:\n",
        "        category = post_type(post_data)\n",
        "        self._analysis_history.append({\"type\": \"classification\", \"result\": category})\n",
        "        return category\n",
        "\n",
        "    # Method 3\n",
        "    def analyze_lengths(self, posts: List[str], category_keywords: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "        results = analyze_post_lengths_by_category(posts, category_keywords)\n",
        "        self._analysis_history.append({\"type\": \"length_analysis\", \"categories\": len(results)})\n",
        "        return results\n",
        "\n",
        "    # Method 4\n",
        "    def metadata_report(self, posts: List[str], category_keywords: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "        summary = self.summarize_metadata()\n",
        "        lengths = self.analyze_lengths(posts, category_keywords)\n",
        "        report = {\"summary\": summary, \"lengths\": lengths}\n",
        "        self._analysis_history.append({\"type\": \"report\", \"result\": \"combined\"})\n",
        "        return report\n"
      ],
      "metadata": {
        "id": "RgT7yzvryAWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}