{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fyas101/Reddit_Group_Project/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDGIjukv4ULD"
      },
      "outputs": [],
      "source": [
        "def categorize_by_tone(post_text):\n",
        "    if not isinstance(post_text, str):\n",
        "        raise TypeError(\"post_text must be a string\")\n",
        "\n",
        "    clean_text = post_text.strip()\n",
        "    if not clean_text:\n",
        "        raise ValueError(\"post_text cannot be empty\")\n",
        "\n",
        "    lower_text = clean_text.lower()\n",
        "\n",
        "    # Basic heuristic keywords and cues\n",
        "    anger_cues = ['!', 'angry', 'hate', 'worst', 'terrible']\n",
        "    sarcasm_cues = ['yeah right', 'sure', 'totally']\n",
        "    humor_cues = ['lol', 'funny', 'haha']\n",
        "    uncertainty_cues = ['maybe', 'not sure', 'idk', 'perhaps']\n",
        "\n",
        "    # Rule-based detection\n",
        "    if any(cue in lower_text for cue in sarcasm_cues):\n",
        "        return 'sarcastic'\n",
        "    elif any(cue in lower_text for cue in anger_cues) and clean_text.isupper():\n",
        "        return 'angry'\n",
        "    elif any(cue in lower_text for cue in humor_cues):\n",
        "        return 'humorous'\n",
        "    elif any(cue in lower_text for cue in uncertainty_cues):\n",
        "        return 'uncertain'\n",
        "    elif '?' in clean_text:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'informative'\n",
        "\n",
        "#Group 3: Organizing\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_misinformation(post_text, keyword_list=None):\n",
        "  if not isinstance(post_text, str):\n",
        "        raise TypeError(\"post_text must be a string.\")\n",
        "  if keyword_list is not None and not (isinstance(keyword_list, list) and all(isinstance(k, str) for k in keyword_list)):\n",
        "        raise TypeError(\"keyword_list must be a list of strings or None.\")\n",
        "\n",
        "  # Default keywords often associated with misinformation\n",
        "  default_keywords = [\n",
        "        \"rumor\", \"unconfirmed\", \"heard\", \"confirmed??\", \"sources say\", \"reportedly\", \"breaking\", \"shocking\",\n",
        "        \"can't believe\", \"conspiracy\", \"fake news\", \"scam\", \"hoax\", \"allegedly\"\n",
        "  ]\n",
        "\n",
        "  keywords = keyword_list if keyword_list else default_keywords\n",
        "\n",
        "  # Clean post text for matching\n",
        "  cleaned_text = post_text.lower()\n",
        "  cleaned_text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", cleaned_text)  # remove URLs\n",
        "  cleaned_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", cleaned_text)  # remove punctuation\n",
        "\n",
        "  matched = [kw for kw in keywords if kw in cleaned_text]\n",
        "\n",
        "  return {\n",
        "        \"is_misinformation\": bool(matched),\n",
        "        \"matched_keywords\": matched\n",
        "  }\n",
        "\n",
        "  #Group 3"
      ],
      "metadata": {
        "id": "FVts1BuD9Nur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_category_keywords(categories):\n",
        "# Returns a dictionary that maps each category to example keywords after given a list of categories.\n",
        "\n",
        "    categegory_keywords = {\n",
        "        \"humor\": [\"lol\", \"lmao\", \"haha\", \"joke\"],\n",
        "        \"random\": [\"idk\", \"random\",],\n",
        "        \"news\": [\"alert\", \"news\", \"update\", \"announcement\", \"diamondback\", \"report\"],\n",
        "        \"academics\": [\"class\", \"classes\", \"exam\", \"professor\", \"grade\", \"gpa\", \"study\", \"midterm\", \"final\", \"project\"],\n",
        "        \"advice\": [\"reccomend\", \"tips\", \"help\", \"should I\", \"question\"],\n",
        "        \"social\": [\"party\", \"hangout\", \"movie\", \"homecoming\", \"game\"],\n",
        "    }\n",
        "# Lists of categories with assigned example keywords\n",
        "    return {cat: extract_category_keywords.get(cat, []) for cat in categories}\n",
        "def normalize_word(word):\n",
        "       # helps to make keywords lowercase as well as clean up extra space\n",
        "        return word.strip().lower()\n",
        "\n",
        "#Group 3"
      ],
      "metadata": {
        "id": "KRgfi7Li9a3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def total_metadata_type(posts_metadata):\n",
        "  if not isinstance(posts_metadata, list):\n",
        "        raise TypeError(\"posts_metadata must be a list of dictionaries\")\n",
        "\n",
        "  if not posts_metadata:\n",
        "        raise ValueError(\"posts_metadata cannot be empty\")\n",
        "\n",
        "  if not all(isinstance(post, dict) for post in posts_metadata):\n",
        "        raise TypeError(\"Each element in posts_metadata must be a dictionary\")\n",
        "\n",
        "  summary = {}\n",
        "\n",
        "  for post in posts_metadata:\n",
        "        for key, value in post.items():\n",
        "            if key not in summary:\n",
        "                summary[key] = {}\n",
        "            str_value = str(value)\n",
        "            summary[key][str_value] = summary[key].get(str_value, 0) + 1\n",
        "\n",
        "  return summary\n",
        "\n",
        "#GROUP 2: Metadata"
      ],
      "metadata": {
        "id": "Jo6TuzIB6ghX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_type(post_data):\n",
        "  if not isinstance(post_data, dict):\n",
        "        raise TypeError(\"post_data must be a dictionary.\")\n",
        "\n",
        "  # If Reddit metadata explicitly says this is an image\n",
        "  if post_data.get(\"post_hint\") == \"image\":\n",
        "        return \"Pictures\"\n",
        "\n",
        "  # If URL ends with an image extension\n",
        "  url = post_data.get(\"url\", \"\").lower()\n",
        "  image_extensions = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\", \".webp\"]\n",
        "  if any(url.endswith(ext) for ext in image_extensions):\n",
        "        return \"Pictures\"\n",
        "\n",
        "  # If text is present\n",
        "  selftext = post_data.get(\"selftext\", \"\")\n",
        "  if isinstance(selftext, str) and selftext.strip():\n",
        "        return \"Words\"\n",
        "\n",
        "  return \"Other\"\n",
        "\n",
        "  #Group 2"
      ],
      "metadata": {
        "id": "UNaXyHV89gnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_post_lengths_by_category(posts, category_keywords):\n",
        "  categorized = {}\n",
        "    # A list of posts and category_keywords dictionary is taken.\n",
        "    # Returns a dictionary where each category has list of the post and length with an average word count.\n",
        "  for post in posts:\n",
        "        if not isinstance(post, str) or not post.strip():\n",
        "            continue\n",
        "# Detects the category of the post based on keywords\n",
        "        category = analyze_post_lengths_by_category(post, category_keywords)\n",
        "# Calculates how many words the post contains\n",
        "        word_count = len(post.split())\n",
        "# Stores post and word count\n",
        "        categorized[category].append((post, word_count))\n",
        "        results = {}\n",
        "# Loops through categories to match (post, length) and extracts word counts\n",
        "  for cat, items in categorized.items():\n",
        "        lengths = [length for _, length in items]\n",
        "        avg_length = round(sum(lengths) / len(lengths), 2)\n",
        "# Stores data of (post, length)\n",
        "        results[cat] = {\n",
        "            \"posts\": items,\n",
        "            \"average_length\": avg_length\n",
        "        }\n",
        "  return results\n",
        "\n",
        "  #Group 2"
      ],
      "metadata": {
        "id": "zjiRT85b9qba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def track_users(posts_data):\n",
        "  if not isinstance(posts_data, list):\n",
        "        raise TypeError(\"posts_data must be a list of dictionaries\")\n",
        "\n",
        "  if not posts_data:\n",
        "        raise ValueError(\"posts_data cannot be empty\")\n",
        "\n",
        "  required_keys = {\"username\", \"tone\", \"category\", \"is_disinformation\", \"upvotes\", \"comments\"}\n",
        "  if not all(required_keys.issubset(post.keys()) for post in posts_data):\n",
        "        raise ValueError(f\"Each post must include keys: {required_keys}\")\n",
        "\n",
        "  users_summary = {}\n",
        "\n",
        "  for post in posts_data:\n",
        "        user = post[\"username\"]\n",
        "        if user not in users_summary:\n",
        "            users_summary[user] = {\n",
        "                \"total_posts\": 0,\n",
        "                \"total_upvotes\": 0,\n",
        "                \"total_comments\": 0,\n",
        "                \"disinformation_posts\": 0,\n",
        "                \"tones_used\": {},\n",
        "                \"categories_posted\": {}\n",
        "            }\n",
        "\n",
        "        summary = users_summary[user]\n",
        "        summary[\"total_posts\"] += 1\n",
        "        summary[\"total_upvotes\"] += post[\"upvotes\"]\n",
        "        summary[\"total_comments\"] += post[\"comments\"]\n",
        "\n",
        "        # Track disinformation\n",
        "        if post[\"is_disinformation\"]:\n",
        "            summary[\"disinformation_posts\"] += 1\n",
        "\n",
        "        # Track tones\n",
        "        tone = post[\"tone\"]\n",
        "        summary[\"tones_used\"][tone] = summary[\"tones_used\"].get(tone, 0) + 1\n",
        "\n",
        "        # Track categories\n",
        "        category = post[\"category\"]\n",
        "        summary[\"categories_posted\"][category] = summary[\"categories_posted\"].get(category, 0) + 1\n",
        "\n",
        "  # Compute averages\n",
        "  for user, stats in users_summary.items():\n",
        "        stats[\"avg_upvotes\"] = round(stats[\"total_upvotes\"] / stats[\"total_posts\"], 2)\n",
        "        stats[\"avg_comments\"] = round(stats[\"total_comments\"] / stats[\"total_posts\"], 2)\n",
        "        del stats[\"total_upvotes\"]\n",
        "        del stats[\"total_comments\"]\n",
        "\n",
        "  return users_summary\n",
        "\n",
        "#GROUP 4: Users"
      ],
      "metadata": {
        "id": "6xIXbCty7Qlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_posters_list(posts_data, top_n=10):\n",
        "   if not isinstance(posts_data, list):\n",
        "        raise TypeError(\"posts_data must be a list of dictionaries\")\n",
        "\n",
        "   if not posts_data:\n",
        "        raise ValueError(\"posts_data cannot be empty\")\n",
        "\n",
        "   required_keys = {\"username\", \"upvotes\", \"comments\", \"is_disinformation\"}\n",
        "   if not all(required_keys.issubset(post.keys()) for post in posts_data):\n",
        "        raise ValueError(f\"Each post must include keys: {required_keys}\")\n",
        "\n",
        "   user_stats = {}\n",
        "\n",
        "   for post in posts_data:\n",
        "        user = post[\"username\"]\n",
        "        if user not in user_stats:\n",
        "            user_stats[user] = {\n",
        "                \"total_posts\": 0,\n",
        "                \"total_engagement\": 0,\n",
        "                \"disinformation_posts\": 0\n",
        "            }\n",
        "\n",
        "        user_stats[user][\"total_posts\"] += 1\n",
        "        user_stats[user][\"total_engagement\"] += post[\"upvotes\"] + post[\"comments\"]\n",
        "        if post[\"is_disinformation\"]:\n",
        "            user_stats[user][\"disinformation_posts\"] += 1\n",
        "\n",
        "    # Compute averages and prepare for sorting\n",
        "   ranked_users = []\n",
        "   for user, stats in user_stats.items():\n",
        "        avg_engagement = round(stats[\"total_engagement\"] / stats[\"total_posts\"], 2)\n",
        "        ranked_users.append({\n",
        "            \"username\": user,\n",
        "            \"total_posts\": stats[\"total_posts\"],\n",
        "            \"total_engagement\": stats[\"total_engagement\"],\n",
        "            \"avg_engagement\": avg_engagement,\n",
        "            \"disinformation_posts\": stats[\"disinformation_posts\"]\n",
        "        })\n",
        "\n",
        "    # Sort by total_engagement first, then total_posts\n",
        "   ranked_users.sort(key=lambda u: (u[\"total_engagement\"], u[\"total_posts\"]), reverse=True)\n",
        "\n",
        "   return ranked_users[:top_n]\n",
        "\n",
        "   #Group 4"
      ],
      "metadata": {
        "id": "r6uoeFky9uu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_post_text(text):\n",
        "  if not isinstance(text, str):\n",
        "        raise TypeError(\"Input must be a string.\")\n",
        "\n",
        "  # Remove URLs\n",
        "  text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "\n",
        "  # Remove Reddit mentions and hashtags\n",
        "  text = re.sub(r\"[@#]\\w+\", \"\", text)\n",
        "\n",
        "  # Remove punctuation and non-alphabetic characters (except spaces)\n",
        "  text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "\n",
        "  # Convert to lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove extra whitespace\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "  return text\n",
        "\n",
        "  #Group 1: cleaning"
      ],
      "metadata": {
        "id": "wp2iPUR89xpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_duplicate(new_post, existing_posts, similarity_threshold=0.9):\n",
        "  if not isinstance(new_post, str):\n",
        "        raise TypeError(\"new_post must be a string.\")\n",
        "  if not isinstance(existing_posts, list) or not all(isinstance(p, str) for p in existing_posts):\n",
        "        raise TypeError(\"existing_posts must be a list of strings.\")\n",
        "  if not (0 <= similarity_threshold <= 1):\n",
        "        raise ValueError(\"similarity_threshold must be between 0 and 1.\")\n",
        "\n",
        "  # --- Helper function: clean text for comparison ---\n",
        "  def _clean_text(text):\n",
        "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text).lower()\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        return text\n",
        "\n",
        "  new_post_clean = _clean_text(new_post)\n",
        "  new_words = set(new_post_clean.split())\n",
        "\n",
        "  for post in existing_posts:\n",
        "        existing_clean = _clean_text(post)\n",
        "        existing_words = set(existing_clean.split())\n",
        "\n",
        "        if not existing_words:\n",
        "            continue\n",
        "\n",
        "        # Calculate similarity: word overlap ratio\n",
        "        overlap = len(new_words & existing_words) / len(new_words | existing_words)\n",
        "\n",
        "        if overlap >= similarity_threshold:\n",
        "            return True  # Duplicate or near-duplicate found\n",
        "\n",
        "  return False\n",
        "\n",
        "  #Group 1"
      ],
      "metadata": {
        "id": "D7G3sL1P-Aq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sorts_weekly_top_10(posts_data):\n",
        "  if not isinstance(posts_data, list):\n",
        "        raise TypeError(\"posts_data must be a list of dictionaries\")\n",
        "\n",
        "  if not posts_data:\n",
        "        raise ValueError(\"posts_data cannot be empty\")\n",
        "\n",
        "  for post in posts_data:\n",
        "        if not isinstance(post, dict):\n",
        "            raise TypeError(\"Each post must be a dictionary\")\n",
        "        if \"timestamp\" not in post or \"upvotes\" not in post or \"comments\" not in post:\n",
        "            raise ValueError(\"Each post must include 'timestamp', 'upvotes', and 'comments' keys\")\n",
        "\n",
        "  # Group posts by ISO week number (YYYY-W##)\n",
        "  weekly_posts = defaultdict(list)\n",
        "\n",
        "  for post in posts_data:\n",
        "        try:\n",
        "            dt = datetime.fromisoformat(post[\"timestamp\"])\n",
        "        except ValueError:\n",
        "            raise ValueError(f\"Invalid timestamp format in post: {post.get('title', 'unknown')}\")\n",
        "\n",
        "        week_label = f\"{dt.isocalendar().year}-W{dt.isocalendar().week:02d}\"\n",
        "\n",
        "        # Add engagement score\n",
        "        post_copy = dict(post)\n",
        "        post_copy[\"engagement\"] = post[\"upvotes\"] + post[\"comments\"]\n",
        "        weekly_posts[week_label].append(post_copy)\n",
        "\n",
        "    # Sort and take top 10 per week\n",
        "  top_weekly_posts = {}\n",
        "  for week, posts in weekly_posts.items():\n",
        "        sorted_posts = sorted(posts, key=lambda p: p[\"engagement\"], reverse=True)\n",
        "        top_weekly_posts[week] = sorted_posts[:10]\n",
        "\n",
        "  return top_weekly_posts\n",
        "  #Group 5"
      ],
      "metadata": {
        "id": "4GHf-1ev8Ahc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def total_interactions_this_week(posts_data, current_date=None):\n",
        "  if not isinstance(posts_data, list):\n",
        "        raise TypeError(\"posts_data must be a list of dictionaries\")\n",
        "\n",
        "  if not posts_data:\n",
        "        return {\n",
        "            \"total_posts\": 0,\n",
        "            \"total_upvotes\": 0,\n",
        "            \"total_comments\": 0,\n",
        "            \"total_interactions\": 0,\n",
        "            \"start_date\": None,\n",
        "            \"end_date\": None\n",
        "        }\n",
        "\n",
        "  required_keys = {\"upvotes\", \"comments\", \"created_utc\"}\n",
        "  if not all(required_keys.issubset(post.keys()) for post in posts_data):\n",
        "        raise TypeError(f\"Each post must include keys: {required_keys}\")\n",
        "\n",
        "  if current_date is None:\n",
        "        current_date = datetime.utcnow()\n",
        "\n",
        "  week_start = current_date - timedelta(days=7)\n",
        "\n",
        "  total_upvotes = 0\n",
        "  total_comments = 0\n",
        "  post_count = 0\n",
        "\n",
        "  for post in posts_data:\n",
        "        try:\n",
        "            post_date = datetime.fromisoformat(post[\"created_utc\"])\n",
        "        except ValueError:\n",
        "            raise ValueError(f\"Invalid date format for post: {post.get('title', 'Unknown')}\")\n",
        "\n",
        "        if week_start <= post_date <= current_date:\n",
        "            post_count += 1\n",
        "            total_upvotes += post[\"upvotes\"]\n",
        "            total_comments += post[\"comments\"]\n",
        "\n",
        "  return {\n",
        "        \"total_posts\": post_count,\n",
        "        \"total_upvotes\": total_upvotes,\n",
        "        \"total_comments\": total_comments,\n",
        "        \"total_interactions\": total_upvotes + total_comments,\n",
        "        \"start_date\": week_start.date().isoformat(),\n",
        "        \"end_date\": current_date.date().isoformat()\n",
        "    }\n",
        "\n",
        "    #Group 5"
      ],
      "metadata": {
        "id": "bDyGeDL5-Sj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interaction_rate(post_data):\n",
        "  if not isinstance(post_data, dict):\n",
        "        raise TypeError(\"post_data must be a dictionary.\")\n",
        "\n",
        "  required_keys = {\"upvotes\", \"comments\", \"views\"}\n",
        "  if not required_keys.issubset(post_data.keys()):\n",
        "        raise TypeError(f\"post_data must include the keys: {required_keys}\")\n",
        "\n",
        "  views = post_data[\"views\"]\n",
        "  if views <= 0:\n",
        "        raise ValueError(\"views must be greater than zero to calculate interaction rate.\")\n",
        "\n",
        "  total_interactions = post_data[\"upvotes\"] + post_data[\"comments\"]\n",
        "  rate = (total_interactions / views) * 100\n",
        "\n",
        "  return round(rate, 2)\n",
        "\n",
        "  #Group 5"
      ],
      "metadata": {
        "id": "rYB461Aj-brN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_weekly_report(posts_data, week_start=None, week_end=None):\n",
        "  if not isinstance(posts_data, list):\n",
        "        raise TypeError(\"posts_data must be a list of dictionaries.\")\n",
        "\n",
        "  # Set default week range if not provided\n",
        "  if week_end is None:\n",
        "        week_end = datetime.utcnow()\n",
        "  if week_start is None:\n",
        "        from datetime import timedelta\n",
        "        week_start = week_end - timedelta(days=7)\n",
        "\n",
        "  # Filter posts in the week\n",
        "  weekly_posts = []\n",
        "  for post in posts_data:\n",
        "        try:\n",
        "            post_date = datetime.fromisoformat(post[\"created_utc\"])\n",
        "            if week_start <= post_date <= week_end:\n",
        "                weekly_posts.append(post)\n",
        "        except Exception:\n",
        "            continue  # skip posts with invalid date formats\n",
        "\n",
        "  # Metrics\n",
        "  total_upvotes = sum(p[\"upvotes\"] for p in weekly_posts)\n",
        "  total_comments = sum(p[\"comments\"] for p in weekly_posts)\n",
        "  total_interactions = total_upvotes + total_comments\n",
        "  total_posts = len(weekly_posts)\n",
        "\n",
        "  # Misinformation detection\n",
        "  misinformation_count = sum(\n",
        "        detect_misinformation(p.get(\"text\", \"\"))[\"is_misinformation\"]\n",
        "        for p in weekly_posts\n",
        "  )\n",
        "  misinformation_rate = round((misinformation_count / total_posts) * 100, 2) if total_posts > 0 else 0\n",
        "\n",
        "  # Top posters\n",
        "  top_posters = top_posters_list(weekly_posts, top_n=5)\n",
        "\n",
        "  # Top posts\n",
        "  top_posts = sorts_weekly_top_10(weekly_posts)\n",
        "\n",
        "  report = {\n",
        "        \"week_start\": week_start.date().isoformat(),\n",
        "        \"week_end\": week_end.date().isoformat(),\n",
        "        \"total_posts\": total_posts,\n",
        "        \"total_upvotes\": total_upvotes,\n",
        "        \"total_comments\": total_comments,\n",
        "        \"total_interactions\": total_interactions,\n",
        "        \"misinformation_count\": misinformation_count,\n",
        "        \"misinformation_rate\": misinformation_rate,\n",
        "        \"top_posters\": top_posters,\n",
        "        \"top_posts\": top_posts\n",
        "  }\n",
        "\n",
        "  return report\n",
        "\n",
        "  #Group 5: Summaries"
      ],
      "metadata": {
        "id": "1w017f8y-emb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_semester_trends(weekly_reports):\n",
        "  if not isinstance(weekly_reports, list) or not all(isinstance(r, dict) for r in weekly_reports):\n",
        "        raise TypeError(\"weekly_reports must be a list of dictionaries.\")\n",
        "\n",
        "  num_weeks = len(weekly_reports)\n",
        "  if num_weeks == 0:\n",
        "        return {}\n",
        "\n",
        "  total_posts_list = [week.get(\"total_posts\", 0) for week in weekly_reports]\n",
        "  total_interactions_list = [week.get(\"total_interactions\", 0) for week in weekly_reports]\n",
        "  misinformation_rate_list = [week.get(\"misinformation_rate\", 0) for week in weekly_reports]\n",
        "\n",
        "  # Average metrics\n",
        "  average_posts_per_week = mean(total_posts_list)\n",
        "  average_interactions_per_week = mean(total_interactions_list)\n",
        "  average_misinformation_rate = mean(misinformation_rate_list)\n",
        "\n",
        "  # Trends for categories and post types\n",
        "  category_counter = Counter()\n",
        "  post_type_counter = Counter()\n",
        "\n",
        "  for week in weekly_reports:\n",
        "        for post in week.get(\"top_posts\", []):\n",
        "            if \"category\" in post:\n",
        "                category_counter[post[\"category\"]] += 1\n",
        "            if \"content_type\" in post:\n",
        "                post_type_counter[post[\"content_type\"]] += 1\n",
        "\n",
        "  category_trends = dict(category_counter.most_common())\n",
        "  most_common_post_types = dict(post_type_counter.most_common())\n",
        "\n",
        "  return {\n",
        "        \"average_posts_per_week\": round(average_posts_per_week, 2),\n",
        "        \"average_interactions_per_week\": round(average_interactions_per_week, 2),\n",
        "        \"average_misinformation_rate\": round(average_misinformation_rate, 2),\n",
        "        \"category_trends\": category_trends,\n",
        "        \"most_common_post_types\": most_common_post_types\n",
        "  }\n",
        "\n",
        "  #Group 5"
      ],
      "metadata": {
        "id": "Qrl-hJk3-nPD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
